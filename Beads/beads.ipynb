{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling probability arrays: a  Python class\n",
    "\n",
    "When working in data science, we often need arrays of weights that sum to one, we call these probability arrays or probability vectors. \n",
    "For example, in classification tasks, we want the output to be a array of probabilities representing predicted class distributions. When implement mixture models, probability arrays can represent the weight of each component. In finance, they might define the weights of assets in a portfolio where the total allocation must sum to one.\n",
    "\n",
    "However, ensuring that these vectors are correctly sampled and always sum to one is not trivial. A simple approach, like sampling random numbers and normalizing them, can lead to uneven or biased distributions. So, how can we guarantee that sampled arrays always meet this constraint?\n",
    "\n",
    "One way to do so is to imagine placing beads into bins. Picture having a finite number of beads, and distributing them across five bins. Each bin represents one component of the probability array, and the fraction of beads in each bin determines the corresponding probability weight. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"https://github.com/PessoaP/blog/blob/master/Beads/beads1.png?raw=true\" alt=\"S\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analogy is not only intuitive but also robust, as it naturally ensures that the total sum is preserved without needing normalization after. Additionally, this representation provides flexibility: by increasing the number of beads, we can achieve higher precision in our probability arrays. Moreover, this bead-and-bin model offers a precise way to represent probabilities. Since beads are discrete, the total count is inherently stable, avoiding the floating-point errors that can arise when dealing with continuous random variables.\n",
    "\n",
    "In this blog post, we build a Python class called `prob_array` to model probability arrays using the bead-and-bin analogy. We'll cover three core functions: (i) initializing arrays from raw counts or by another probability vector, (ii) proposing symmetric updates by redistributing beads; and (iii) calculating the log-probability under a multinomial prior, including necessary mathematical corrections. \n",
    "By the end, you'll have a clean implementation of `prob_array`, with examples showcasing initialization, symmetric proposals, and probabilistic evaluation. Letâ€™s begin with array initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "normalize = lambda x: x/x.sum()\n",
    "\n",
    "class prob_array:\n",
    "    def __init__(self,array=None,components=20,beads=10000):\n",
    "        if array is None:\n",
    "            self.counts = np.ones(components,dtype=int)*(beads//components)\n",
    "            self.counts[:beads%components]+=1\n",
    "        elif array.dtype == int:\n",
    "            self.counts = array\n",
    "        elif np.isclose(1.,np.sum(array)):\n",
    "            self.counts = np.floor(array*beads).astype(int)\n",
    "            self.counts[np.argsort(array)[:beads%self.counts.sum()]]+=1\n",
    "\n",
    "        self.prob = normalize(self.counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposal(p_array,rate=.01):\n",
    "    movables = np.random.binomial(p_array.counts,rate)\n",
    "    new_counts = p_array.counts - movables \n",
    "\n",
    "    mvleft = np.random.binomial(movables,.5)\n",
    "    mvright = movables - mvleft \n",
    "\n",
    "    new_counts[:-1] += mvleft[1:]\n",
    "    new_counts[0] += mvleft[0] #the ones selected to move left from 0 stay in place\n",
    "\n",
    "    new_counts[1:] += mvright[:-1]\n",
    "    new_counts[-1] += mvright[-1] #the ones selected to move left from -1 stay in place\n",
    "\n",
    "    return prob_array(new_counts,new_counts.size,new_counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import loggamma\n",
    "logfactorial = lambda x: loggamma(x+1)    \n",
    "def multinomial_logprob(p_array,alpha):\n",
    "    beads = p_array.counts.sum()\n",
    "    p = p_array.alpha/p_array.alpha.sum()\n",
    "\n",
    "    log_prefactor = p_array.counts.size*np.log(beads) + logfactorial(beads) - logfactorial(p_array.counts).sum()\n",
    "    return log_prefactor + (p_array.counts*np.log(p)).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import loggamma\n",
    "\n",
    "# Utility functions\n",
    "normalize = lambda x: x / x.sum()  # Ensures the input array sums to 1\n",
    "logfactorial = lambda x: loggamma(x + 1)  # Computes the log-factorial using log-gamma\n",
    "\n",
    "class prob_array:\n",
    "    def __init__(self, array=None, components=20, beads=10000):\n",
    "        \"\"\"\n",
    "        Initialize a probability array.\n",
    "        \n",
    "        Parameters:\n",
    "        - array: Optional numpy array, can represent counts or probabilities.\n",
    "        - components: Number of components in the probability array.\n",
    "        - beads: Total number of beads (samples) for normalization.\n",
    "        \"\"\"\n",
    "        if array is None:\n",
    "            # Uniform distribution of beads across components\n",
    "            self.counts = np.ones(components, dtype=int) * (beads // components)\n",
    "            self.counts[:beads % components] += 1  # Distribute remaining beads\n",
    "        elif array.dtype == int:\n",
    "            # If array represents counts directly\n",
    "            self.counts = array\n",
    "        elif np.isclose(1.0, np.sum(array)):\n",
    "            # If array represents probabilities\n",
    "            self.counts = np.floor(array * beads).astype(int)\n",
    "            remainder = beads - self.counts.sum()\n",
    "            self.counts[np.argsort(array)[-remainder:]] += 1  # Adjust for rounding errors\n",
    "        else:\n",
    "            raise ValueError(\"Input array must be counts (int) or probabilities (sum to 1).\")\n",
    "        \n",
    "        self.prob = normalize(self.counts)  # Normalize counts to probabilities\n",
    "\n",
    "    def proposal(self, rate=0.01):\n",
    "        \"\"\"\n",
    "        Generate a proposal for a new probability array.\n",
    "        \n",
    "        Parameters:\n",
    "        - rate: Rate of change for beads redistribution.\n",
    "        \n",
    "        Returns:\n",
    "        - New prob_array instance with adjusted counts.\n",
    "        \"\"\"\n",
    "        movables = np.random.binomial(self.counts, rate)  # Determine movable beads\n",
    "        new_counts = self.counts - movables\n",
    "\n",
    "        # Redistribute beads left and right\n",
    "        mvleft = np.random.binomial(movables, 0.5)\n",
    "        mvright = movables - mvleft\n",
    "\n",
    "        new_counts[:-1] += mvleft[1:]\n",
    "        new_counts[0] += mvleft[0]  # Beads attempting to move left from the first index stay\n",
    "\n",
    "        new_counts[1:] += mvright[:-1]\n",
    "        new_counts[-1] += mvright[-1]  # Beads attempting to move right from the last index stay\n",
    "\n",
    "        # Ensure counts remain valid\n",
    "        if np.any(new_counts < 0):\n",
    "            raise ValueError(\"Invalid move: Negative counts detected.\")\n",
    "        \n",
    "        return prob_array(new_counts, new_counts.size, new_counts.sum())\n",
    "    \n",
    "    def multinomial_logprob(self, alpha):\n",
    "        \"\"\"\n",
    "        Compute the log-probability of the current counts given a multinomial distribution.\n",
    "        \n",
    "        Parameters:\n",
    "        - alpha: Dirichlet prior (numpy array).\n",
    "        \n",
    "        Returns:\n",
    "        - Log-probability value.\n",
    "        \"\"\"\n",
    "        beads = self.counts.sum()\n",
    "        if len(alpha) != len(self.counts):\n",
    "            raise ValueError(\"Alpha vector must have the same length as counts.\")\n",
    "        \n",
    "        p = alpha / alpha.sum()  # Normalize alpha to get probabilities\n",
    "        \n",
    "        log_prefactor = logfactorial(beads) - logfactorial(self.counts).sum() + self.counts.size * np.log(beads)\n",
    "        return log_prefactor + (self.counts * np.log(p)).sum()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"prob_array(counts={self.counts}, prob={self.prob})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
