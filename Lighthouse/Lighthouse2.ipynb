{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2292c3",
   "metadata": {},
   "source": [
    "# Why do we need Bayesian statistics? Part III -- Learning multivariate distributions (tutorial)\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f0018",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the previous [entry of this tutorial series](https://labpresse.com/why-do-we-need-bayesian-statistics-part-ii-the-lighthouse-problem-tutorial/), we studied the lighthouse problem. The goal was to infer (or learn) the position of the lighthouse from observations of where the lighthouse's light beam hits the floor. A schematic of the problem is presented below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f34e3a",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"TikZfig-12.png\" alt=\"Alt text for image\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace3b2a2",
   "metadata": {},
   "source": [
    "Following what was established on the previous post, let us generate synthetic data for the lighhouse problem. We sample $\\theta$ uniformly in $\\left( -\\frac{\\pi}{2}, \\frac{\\pi}{2} \\right)$ and obtain $x$ as\n",
    "\n",
    "$$ tan \\theta = \\frac{x-l_x}{l_h} \\Rightarrow x = l_x + l_h tan \\theta \\ .  \\quad \\quad \\quad (1) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b8c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(42)\n",
    "\n",
    "l_x,l_h = 0,1 #setting ground truth\n",
    "\n",
    "th = np.pi*(np.random.rand(1024)-1/2) #sample theta\n",
    "x = l_x +l_h*np.tan(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628517ad",
   "metadata": {},
   "source": [
    "In the previous [post](https://labpresse.com/why-do-we-need-bayesian-statistics-part-ii-the-lighthouse-problem-tutorial/) we outline how to infer $l_x$, the lighthouse horizontal position, assuming the height, $l_h$ is already known. However, the study of probabilities presented in that post does allow one to infer both $l_x$ and $l_h$ despite the fact that the observation $x$ is univariate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0b146",
   "metadata": {},
   "source": [
    "### The likelihood\n",
    "\n",
    "This is possible by obtaining the conditional probability of a single point $x$ given $l_x$ and $l_h$, as done in the [previous blog post](https://labpresse.com/why-do-we-need-bayesian-statistics-part-ii-the-lighthouse-problem-tutorial/), which is given by\n",
    "$$p(x|l_x,l_h) = \\frac{1}{\\pi} \\frac{l_h}{ (x-l_x)^2 + l_h^2 }  \\ . \\quad \\quad \\quad (2)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000cdf7",
   "metadata": {},
   "source": [
    "For a set of multiple data points  $\\{x_n\\} = \\{x_1, x_2,x_3, \\ldots , x_N\\}$ obtained independently, the conditional probability, or likelihood, of the whole dataset is\n",
    "$$ p(\\{x_n\\}|l_x,l_h) = \\prod_{n=1}^N \\left[ \\frac{1}{\\pi} \\frac{l_h}{ (x_n-l_x)^2 + l_h^2 }  \\right] \\ . \\quad \\quad \\quad (3)  $$\n",
    "Which, as in the  [previous blog post](https://labpresse.com/why-do-we-need-bayesian-statistics-part-ii-the-lighthouse-problem-tutorial/) is expressed in terms of the logarithm for numerical stability, yielding\n",
    "$$\\ln p(\\{x_n\\}|l_x,l_h) = \\sum_{n=1}^N \\ln \\left[ \\frac{1}{\\pi} \\frac{l_h}{ (x_n-l_x)^2 + l_h^2 }  \\right]  . \\quad \\quad \\quad (4)  $$\n",
    "which can be calculated using the following piece of code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0b5b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_like_2d(x,lx,lh):\n",
    "    LX,LH=np.meshgrid(lx,lh) \n",
    "    log_like = np.zeros_like(LX)\n",
    "    for xi in x:\n",
    "        log_like += np.log(LH)-np.log(np.pi)-np.log((xi-LX)**2+LH**2)\n",
    "    return log_like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd535f1",
   "metadata": {},
   "source": [
    "Here we assume that $l_x$ and $l_h$ were given in a form of an (`numpy`) array representing all possible values. The choice to represent this way is better explained when we look into the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7068b9",
   "metadata": {},
   "source": [
    "### Prior\n",
    "In order to infer $l_x$ and $l_h$ we ought to invert the conditional probability using Bayes' theorem:\n",
    "$$p(l_x,l_h|\\{x_n\\}) = \\frac{ p(\\{x_n\\}|l_x,l_h)}{p(\\{x_n\\})}    p(l_x,l_h) \\ , \\quad \\quad \\quad (5)$$  \n",
    "\n",
    "with the prior, $p(l_x,l_h)$, chosen in a manner similar to the [previous blog post](https://labpresse.com/why-do-we-need-bayesian-statistics-part-ii-the-lighthouse-problem-tutorial/) \n",
    "$$\n",
    "p(l_x,l_h) = \n",
    "\\begin{cases}\n",
    "\\frac{1}{LH} & \\text{for} \\  -\\frac{L}{2}<l_x<\\frac{L}{2} \\ \\text{and} \\ 0< l_h< H \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases} \\ .  \\quad \\quad \\quad (6)\n",
    "$$\n",
    "Or, in other words, the prior is uniform in  $l_x \\in (-\\frac{L}{2},\\frac{L}{2})$  and $l_h \\in (0,H)$. For the purposes of later visualization we choose $L = H = 2$, but the results allow for much larger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b8382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "L,H = 2.,2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c440b",
   "metadata": {},
   "source": [
    "First let us generate the arrays for $l_x$ and $l_h$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85bbc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_arr=np.linspace(0,H,101)+1e-12\n",
    "lx_arr=np.linspace(-L/2,L/2,101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51fd58c",
   "metadata": {},
   "source": [
    "this guarantees a good representation of the intervals  $l_x \\in (-\\frac{L}{2},\\frac{L}{2})$  and $l_h \\in (0,H)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61189675",
   "metadata": {},
   "source": [
    "### Posterior\n",
    "\n",
    "Going back to the Bayes' theorem (5), and substituting the prior (6) and likelihood (4) one obtains \n",
    "$$ p(l_x,l_h|\\{x_n\\}) = \\frac{1}{Z'} \\ \\exp\\left[  \\sum_{n=1}^N \\ln \\left[ \\frac{1}{\\pi} \\frac{l_h}{ (x_n-l_x)^2 + l_h^2 }  \\right]   \\right]  \\ . \\quad \\quad \\quad (7)$$  \n",
    "Where $Z' = LH P(\\{x_n\\})$, note that $P(\\{x_n\\})$ in (5) does not depend on $l_x$ or $l_h$. As such, $Z'$ is calculares by ensuring that $\\int \\mathrm{d} l_h \\  \\mathrm{d} l_x \\  p(l_x,l_h|\\{x_n\\}) = 1$. \n",
    "The following block of code gives a function to calculate the posterior (7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01fe8e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(x,lx,lh):\n",
    "    L = log_like_2d(x,lx,lh)\n",
    "    L -= L.max()\n",
    "\n",
    "    P = np.exp(L) #unnormalized posterior\n",
    "    \n",
    "    dlh,dlx = (lh[1]-lh[0]),(lx[1]-lx[0])\n",
    "    Z = P.sum()*(dlh*dlx) #normalization factor\n",
    "\n",
    "    P = P/Z\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d89745",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86055324",
   "metadata": {},
   "source": [
    "Now that we have the computational tools to calculate the posterior let us visualize how the posterior evolves with the number of datapoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c612e",
   "metadata": {},
   "source": [
    "Since we are talking about a two dimensional posterior, we present how it evolves with the number of datapoints through an animation (see the complete code in my [GitHub](https://github.com/PessoaP/blog) repository)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2944b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40251/38688199.py:43: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  imageio.mimsave(output_gif, [imageio.imread(frame) for frame in frames], duration=.5)\n"
     ]
    }
   ],
   "source": [
    "#Removed data block, see notebook on GitHub for full code\n",
    "import imageio\n",
    "\n",
    "Ns = [10,20,50,100,200,500,1000]\n",
    "Ns = list(np.arange(10,100,10))+list(np.arange(100,1001,100))\n",
    "frames=[]\n",
    "\n",
    "for i in range(len(Ns)):\n",
    "    fig,ax =plt.subplots()\n",
    "    lim = Ns[i]\n",
    "    x_eff = x[:lim]\n",
    "    dlh,dlx = (lh_arr[1]-lh_arr[0]),(lx_arr[1]-lx_arr[0])\n",
    "    X,Y = np.meshgrid(lx_arr,lh_arr)\n",
    "    P = posterior(x_eff,lx_arr,lh_arr)\n",
    "\n",
    "    CS = ax.contourf(X,Y,P)\n",
    "    CF = ax.contour(X,Y,P,colors='k')\n",
    "    cbar = plt.colorbar(CS, ax=ax)\n",
    "\n",
    "\n",
    "    ax.set_ylabel(r'Lighthouse height, $l_h$',fontsize=15)\n",
    "    ax.set_xlabel(r'Lighthouse horizontal location, $l_x$',fontsize=15)\n",
    "    ax.set_title('{:4d} datapoints'.format(lim),fontsize=15)\n",
    "    ax.set_yticks(np.linspace(0,2,5))\n",
    "    ax.set_xticks(np.linspace(-1,1,5))\n",
    "\n",
    "    ax.scatter(l_x,l_h,color='r',label='Target (Ground Truth)')\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "\n",
    "    filename = f'gif_frames/frame_{i}.png'\n",
    "    plt.savefig(filename)\n",
    "    frames.append(filename)\n",
    "\n",
    "    cbar.remove()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "output_gif = 'contour_animation.gif'\n",
    "imageio.mimsave(output_gif, [imageio.imread(frame) for frame in frames], duration=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e61833",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"contour_animation.gif\" alt=\"Alt text for image\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743d340",
   "metadata": {},
   "source": [
    "### Marginal posterior distribution\n",
    "While in the [previous blog post](https://labpresse.com/why-do-we-need-bayesian-statistics-part-ii-the-lighthouse-problem-tutorial/)  we found the distribution for the lighthouse horizontal position, $l_x$, assuming a known height, $l_h$, here we obtain the distribution for both the $l_h$ and $l_x$, denoted as $p(l_x,l_h|\\{x_n\\})$. In order to obtain the posterior for only $l_x$ or only for $l_h$ we ought to sum (or integrate) over the probabilities of all possible value of the other parameter, that is: \n",
    "$$ p(l_x|\\{x_n\\}) = \\int \\mathrm{d}l_h  \\ p(l_x,l_h|\\{x_n\\}) \\ , \\quad \\quad \\quad (8a) $$\n",
    "$$ p(l_h|\\{x_n\\}) = \\int \\mathrm{d}l_x  \\ p(l_x,l_h|\\{x_n\\}) \\ . \\quad \\quad \\quad (8b) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb0150",
   "metadata": {},
   "source": [
    "In probability theory obtaining the probability for one parameter from the integral over the other parameters as above is often referred as marginalization. Moreover, in Bayesian language $p(l_x|\\{x_n\\})$ and $p(l_h|\\{x_n\\})$ are the marginalized posteriors while $p(l_x,l_h|\\{x_n\\})$ is the joint posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bbc9de",
   "metadata": {},
   "source": [
    "It is important to notice that $p(l_x|\\{x_n\\})$ obtained above is conceptually different for what was done in the [previous blog post](https://labpresse.com/why-do-we-need-bayesian-statistics-part-ii-the-lighthouse-problem-tutorial/). While there we considered that $l_h$ was already known here we cover all possible values of $l_h$ based on the posterior, which was learned from data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063fa809",
   "metadata": {},
   "source": [
    "In order to give a good visualization of how the marginal posterior for both $l_x$ and $l_h$  evolve with the total number of data points, we show an animation analogous to the previous one but also presenting the marginal posteriors parallel to their respective axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966eab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40251/3973247083.py:61: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  imageio.mimsave(output_gif, [imageio.imread(frame) for frame in frames], duration=.4)\n"
     ]
    }
   ],
   "source": [
    "#Removed data block, see notebook on GitHub for full code\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "frames=[]\n",
    "\n",
    "for i in range(len(Ns)):\n",
    "    lim = Ns[i]\n",
    "    x_eff = x[:lim]\n",
    "    dlh,dlx = (lh_arr[1]-lh_arr[0]),(lx_arr[1]-lx_arr[0])\n",
    "    X,Y = np.meshgrid(lx_arr,lh_arr)\n",
    "    P = posterior(x_eff,lx_arr,lh_arr)\n",
    "\n",
    "    # Create a 2x2 grid of subplots\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[4, 1], height_ratios=[1, 4])\n",
    "\n",
    "    # Central Contour Plot\n",
    "    ax_center = plt.subplot(gs[1, 0])\n",
    "    ax_center.contourf(X,Y,P)\n",
    "    ax_center.contour(X,Y,P,colors='k')\n",
    "    ax_center.scatter(l_x,l_h,color='r',label='Target (Ground Truth)')\n",
    "    ax_center.set_ylabel(r'Lighthouse height, $l_h$',fontsize=15)\n",
    "    ax_center.set_xlabel(r'Lighthouse horizontal location, $l_x$',fontsize=15)\n",
    "    ax_center.set_yticks(np.linspace(0,2,5))\n",
    "    ax_center.set_xticks(np.linspace(-1,1,5))\n",
    "\n",
    "\n",
    "    # side Histogram (X-axis)\n",
    "    ax_side = plt.subplot(gs[1, 1])\n",
    "    ax_side.plot(P.sum(axis=1)*dlx,lh_arr)\n",
    "    ax_side.set_xlabel('Marginal posterior')\n",
    "    #ax_side.set_ylabel(r'$l_h$',fontsize=15)\n",
    "    ax_side.axhline(l_h,color='r',label='Ground Truth')\n",
    "    ax_side.set_yticks(np.linspace(0,2,5))\n",
    "    ax_side.set_ylim((0,2))\n",
    "    #ax_side.invert_xaxis()\n",
    "    ax_side.set_title('')\n",
    "\n",
    "\n",
    "    # bottom Histogram (Y-axis)\n",
    "    ax_bottom = plt.subplot(gs[0, 0])\n",
    "    ax_bottom.plot(lx_arr,P.sum(axis=0)*dlh)\n",
    "    #ax_bottom.set_xlabel(r'$l_x$',fontsize=15)\n",
    "    ax_bottom.set_ylabel('Marginal posterior')\n",
    "    ax_bottom.axvline(l_x,color='r',label='Ground Truth')\n",
    "    ax_bottom.set_xticks(np.linspace(-1,1,5))\n",
    "    ax_bottom.set_xlim((-1,1))\n",
    "    \n",
    "    fig.suptitle('{:4d} datapoints'.format(lim),fontsize=20)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    \n",
    "    filename = f'gif_frames/marginal_frame-{i}.png'\n",
    "    plt.savefig(filename)\n",
    "    frames.append(filename)\n",
    "    plt.close()\n",
    "\n",
    "output_gif = 'marginal_contour_animation.gif'\n",
    "imageio.mimsave(output_gif, [imageio.imread(frame) for frame in frames], duration=.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5588b",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"marginal_contour_animation.gif\" alt=\"Alt text for image\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc65a026",
   "metadata": {},
   "source": [
    "\n",
    "Here we can see clearly that, as the number of data points increase, the posterior (both the joint and each of the marginalized ones) tightens around the ground truth values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a1b2b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "Here we finish the overarching tutorial on Bayesian statistics. While future posts involving the topic will come, we presented a tutorial where:\n",
    "\n",
    "1. [Bayesian statistics concurs with more naive methods of inference,](https://labpresse.com/why-do-we-need-bayesian-statistics-part-i-asserting-if-a-coin-is-biased-tutorial/) \n",
    "2. [The system is severely more complex and these naive methods fail](https://labpresse.com/why-do-we-need-bayesian-statistics-part-ii-the-lighthouse-problem-tutorial/), and\n",
    "3. The present post, where we went further into the previous problems to show how to learn multiple values even when the data available is single-valued.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1516a91a",
   "metadata": {},
   "source": [
    "Although this is the last entry of this tutorial series, more future posts related to the intricacies of probability and statistics in this blog. I hope this was instructive. Stay tuned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
